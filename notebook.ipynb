{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptjovjZH5UNe"
   },
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "rJvK3AK4iRCQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozCpTxcOiRCW",
    "outputId": "b72bcfd7-2e3c-49b6-8ca5-a0941e6750cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbiGyAtK5d8-"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H1ND5fbm062p"
   },
   "outputs": [],
   "source": [
    "import torch  # pytorch\n",
    "\n",
    "# import required torchtext modules\n",
    "from torchtext import data\n",
    "from torch.autograd import Variable\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# animated progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# load the datasets as dataframes\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# concat training and testing datasets\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kea8cSh5if4"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "klnBGw_IiRDA"
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"Remove URLs from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to remove URLs from.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with URLs removed.\n",
    "    \"\"\"\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(\"\", text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"Remove HTML tags from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to remove HTML tags from.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(\"\", text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    \"\"\"Remove emojis from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to remove emojis from.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with emojis removed.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"Remove punctuation from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to remove punctuation from.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with punctuation removed.\n",
    "    \"\"\"\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"Lemmatize words in a sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Sentence to lemmatize.\n",
    "\n",
    "    Returns:\n",
    "        str: Lemmatized sentence.\n",
    "    \"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    sentence_words = sentence.split(\" \")\n",
    "    new_sentence_words = []\n",
    "    for sentence_word in sentence_words:\n",
    "        sentence_word = sentence_word.replace(\"#\", \"\")\n",
    "        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n",
    "        new_sentence_words.append(new_sentence_word)\n",
    "    new_sentence = \" \".join(new_sentence_words)\n",
    "    return new_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6JjMLP4qiRDF"
   },
   "outputs": [],
   "source": [
    "# Apply the remove_URL function to the 'text' column of the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_URL(x))\n",
    "\n",
    "# Apply the remove_html function to the 'text' column of the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))\n",
    "\n",
    "# Apply the remove_emoji function to the 'text' column of the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# Apply the remove_punct function to the 'text' column of the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Apply the lemmatize_sentence function to the 'text' column of the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kWdYvM5BiRDN"
   },
   "outputs": [],
   "source": [
    "def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Split the train set into train and validation set and prepare csv files for the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df_train (DataFrame): The train set as pandas DataFrame.\n",
    "    - df_test (DataFrame): The test set as pandas DataFrame.\n",
    "    - seed (int): Random seed used to shuffle the train set.\n",
    "    - val_ratio (float): The validation set size ratio.\n",
    "\n",
    "    Returns:\n",
    "    None.\n",
    "    \"\"\"\n",
    "    # Shuffle the train set.\n",
    "    idx = np.arange(df_train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    # Compute the validation set size.\n",
    "    val_size = int(len(idx) * val_ratio)\n",
    "\n",
    "    # Create the cache directory if it doesn't exist.\n",
    "    if not os.path.exists(\"cache\"):\n",
    "        os.makedirs(\"cache\")\n",
    "\n",
    "    # Save the train set into a csv file.\n",
    "    df_train.iloc[idx[val_size:], :][[\"id\", \"target\", \"text\"]].to_csv(\n",
    "        \"cache/dataset_train.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Save the validation set into a csv file.\n",
    "    df_train.iloc[idx[:val_size], :][[\"id\", \"target\", \"text\"]].to_csv(\n",
    "        \"cache/dataset_val.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Save the test set into a csv file.\n",
    "    df_test[[\"id\", \"text\"]].to_csv(\"cache/dataset_test.csv\", index=False)\n",
    "\n",
    "\n",
    "def get_iterator(dataset, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    \"\"\"\n",
    "    Create a torchtext iterator for the specified dataset.\n",
    "\n",
    "    Args:\n",
    "    - dataset: The dataset to iterate over.\n",
    "    - batch_size (int): The batch size.\n",
    "    - train (bool): Whether the iterator is used for training or not.\n",
    "    - shuffle (bool): Whether to shuffle the data or not.\n",
    "    - repeat (bool): Whether to repeat the iterator for multiple epochs or not.\n",
    "\n",
    "    Returns:\n",
    "    A torchtext iterator over the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return data.Iterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train=train,\n",
    "        shuffle=shuffle,\n",
    "        repeat=repeat,\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset(fix_length=100, lower=False, vectors=None):\n",
    "    \"\"\"\n",
    "    Load and prepare the dataset for training and testing the model\n",
    "    Args:\n",
    "        fix_length (int): the maximum length of each text\n",
    "        lower (bool): whether to convert all text to lowercase or not\n",
    "        vectors: pretrained word embeddings, default is None\n",
    "\n",
    "    Returns:\n",
    "        tuple: TEXT (data.field.Field), vocab_size (int), word_embeddings (torch.Tensor),\n",
    "            train_iter (torchtext.data.Iterator), val_iter (torchtext.data.Iterator),\n",
    "            test_iter (torchtext.data.Iterator)\n",
    "    \"\"\"\n",
    "    if vectors is not None:\n",
    "        lower = True\n",
    "\n",
    "    # Prepare the CSV files\n",
    "    prepare_csv(train, test)\n",
    "\n",
    "    # Define the fields\n",
    "    TEXT = data.field.Field(\n",
    "        sequential=True,\n",
    "        lower=lower,\n",
    "        include_lengths=True,\n",
    "        batch_first=True,\n",
    "        fix_length=fix_length,\n",
    "    )\n",
    "    LABEL = data.field.Field(use_vocab=True, sequential=False, dtype=torch.float16)\n",
    "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)\n",
    "\n",
    "    # Load the datasets\n",
    "    train_temp, val_temp = data.TabularDataset.splits(\n",
    "        path=\"cache/\",\n",
    "        format=\"csv\",\n",
    "        skip_header=True,\n",
    "        train=\"dataset_train.csv\",\n",
    "        validation=\"dataset_val.csv\",\n",
    "        fields=[(\"id\", ID), (\"target\", LABEL), (\"text\", TEXT)],\n",
    "    )\n",
    "\n",
    "    test_temp = data.TabularDataset(\n",
    "        path=\"cache/dataset_test.csv\",\n",
    "        format=\"csv\",\n",
    "        skip_header=True,\n",
    "        fields=[(\"id\", ID), (\"text\", TEXT)],\n",
    "    )\n",
    "\n",
    "    # Build the vocabulary\n",
    "    TEXT.build_vocab(\n",
    "        train_temp,\n",
    "        val_temp,\n",
    "        test_temp,\n",
    "        max_size=20000,\n",
    "        min_freq=10,\n",
    "        vectors=GloVe(name=\"6B\", dim=300),\n",
    "    )\n",
    "\n",
    "    LABEL.build_vocab(train_temp)\n",
    "\n",
    "    ID.build_vocab(train_temp, val_temp, test_temp)\n",
    "\n",
    "    # Get the iterators for training, validation, and testing\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "    train_iter = get_iterator(\n",
    "        train_temp, batch_size=32, train=True, shuffle=True, repeat=False\n",
    "    )\n",
    "    val_iter = get_iterator(\n",
    "        val_temp, batch_size=32, train=True, shuffle=True, repeat=False\n",
    "    )\n",
    "    test_iter = get_iterator(\n",
    "        test_temp, batch_size=32, train=False, shuffle=False, repeat=False\n",
    "    )\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BEMc4qjLiRDX"
   },
   "outputs": [],
   "source": [
    "# assigns the outputs of the get_dataset() function to the variables\n",
    "TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seMspb6e5qMy"
   },
   "source": [
    "## Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5mu4FUn14PHD"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LSTMClassifier model.\n",
    "\n",
    "        Args:\n",
    "        - vocab_size (int): The size of the vocabulary.\n",
    "        - output_size (int): The size of the output layer.\n",
    "        - embedding_dim (int): The dimensionality of the word embeddings.\n",
    "        - hidden_dim (int): The number of units in the hidden layer.\n",
    "        - n_layers (int): The number of LSTM layers.\n",
    "        - weights (torch.Tensor): The pre-trained word embeddings.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.3)\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            embedding_dim, hidden_dim, n_layers, dropout=0.3, batch_first=True\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout(0.3)\n",
    "        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the LSTMClassifier model.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input tensor of shape (batch_size, seq_len).\n",
    "        - hidden (tuple(torch.Tensor)): The hidden state and cell state of the LSTM.\n",
    "\n",
    "        Returns:\n",
    "        - out (torch.Tensor): The output tensor of shape (batch_size, output_size).\n",
    "        - hidden (tuple(torch.Tensor)): The hidden state and cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.dropout_1(x)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout_2(lstm_out)\n",
    "        out = self.label_layer(out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.act(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state and cell state of the LSTM.\n",
    "\n",
    "        Args:\n",
    "        - batch_size (int): The size of the batch.\n",
    "\n",
    "        Returns:\n",
    "        - hidden (tuple(torch.Tensor)): The hidden state and cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(\n",
    "            device\n",
    "        ), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFUyiTj-6Lyi"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wv7TkTBkiRDf"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the LSTM classifier model.\n",
    "\n",
    "    Args:\n",
    "        model: The LSTMClassifier model to train.\n",
    "        train_iter: The training data iterator.\n",
    "        val_iter: The validation data iterator.\n",
    "        optim: The optimizer to use for training.\n",
    "        loss: The loss function to use for training.\n",
    "        num_epochs: The number of epochs to train for.\n",
    "        batch_size: The batch size to use for training. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        total_train_epoch_loss: A list containing the training loss for each epoch.\n",
    "        total_train_epoch_acc: A list containing the training accuracy for each epoch.\n",
    "        total_val_epoch_loss: A list containing the validation loss for each epoch.\n",
    "        total_val_epoch_acc: A list containing the validation accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    h = model.init_hidden(batch_size)\n",
    "    clip = 5\n",
    "    val_loss_min = np.Inf\n",
    "    total_train_epoch_loss = []\n",
    "    total_train_epoch_acc = []\n",
    "    total_val_epoch_loss = []\n",
    "    total_val_epoch_acc = []\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "        train_epoch_acc = []\n",
    "        val_epoch_loss = []\n",
    "        val_epoch_acc = []\n",
    "        for _, batch in enumerate(tqdm(train_iter)):\n",
    "            h = tuple([e.data for e in h])\n",
    "            text = batch.text[0]\n",
    "            target = batch.target\n",
    "            target = target - 1\n",
    "            target = target.type(torch.LongTensor)\n",
    "            text = text.to(device)\n",
    "            target = target.to(device)\n",
    "            optim.zero_grad()\n",
    "\n",
    "            if text.size()[0] is not batch_size:\n",
    "                continue\n",
    "\n",
    "            prediction, h = model(text, h)\n",
    "            loss_train = loss(prediction.squeeze(), target)\n",
    "            loss_train.backward()\n",
    "            num_corrects = (\n",
    "                (torch.max(prediction, 1)[1].view(target.size()).data == target.data)\n",
    "                .float()\n",
    "                .sum()\n",
    "            )\n",
    "\n",
    "            acc = 100.0 * num_corrects / len(batch)\n",
    "            train_epoch_loss.append(loss_train.item())\n",
    "            train_epoch_acc.append(acc.item())\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "\n",
    "        print(\n",
    "            f\"Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%\"\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(tqdm(val_iter)):\n",
    "                val_h = tuple([e.data for e in h])\n",
    "                text = batch.text[0]\n",
    "                target = batch.target\n",
    "                target = target - 1\n",
    "                target = target.type(torch.LongTensor)\n",
    "                text = text.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                if text.size()[0] is not batch_size:\n",
    "                    continue\n",
    "\n",
    "                prediction, h = model(text, h)\n",
    "                loss_val = loss(prediction.squeeze(), target)\n",
    "                num_corrects = (\n",
    "                    (\n",
    "                        torch.max(prediction, 1)[1].view(target.size()).data\n",
    "                        == target.data\n",
    "                    )\n",
    "                    .float()\n",
    "                    .sum()\n",
    "                )\n",
    "\n",
    "                acc = 100.0 * num_corrects / len(batch)\n",
    "                val_epoch_loss.append(loss_val.item())\n",
    "                val_epoch_acc.append(acc.item())\n",
    "\n",
    "            print(\n",
    "                f\"Vadlidation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%\"\n",
    "            )\n",
    "\n",
    "            if np.mean(val_epoch_loss) <= val_loss_min:\n",
    "                print(\n",
    "                    \"Validation loss decreased ({:.6f} --> {:.6f})\".format(\n",
    "                        val_loss_min, np.mean(val_epoch_loss)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                val_loss_min = np.mean(val_epoch_loss)\n",
    "\n",
    "        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n",
    "        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n",
    "        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n",
    "        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n",
    "\n",
    "    return (\n",
    "        total_train_epoch_loss,\n",
    "        total_train_epoch_acc,\n",
    "        total_val_epoch_loss,\n",
    "        total_val_epoch_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXPjaSmWiRDh"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "hidden_size = 128\n",
    "embedding_length = 300\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    output_size=output_size,\n",
    "    embedding_dim=embedding_length,\n",
    "    hidden_dim=hidden_size,\n",
    "    n_layers=2,\n",
    "    weights=word_embeddings,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss, train_acc, val_loss, val_acc = train_model(\n",
    "    model=model,\n",
    "    train_iter=train_iter,\n",
    "    val_iter=val_iter,\n",
    "    optim=optim,\n",
    "    loss=loss,\n",
    "    num_epochs=20,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrU4U8guiRDo"
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c9301a7b09704f418d642abd507535b4",
      "ed98ee7ab5a34fb9842ec3f5f04c81a9",
      "bea7dae81cf84737a5ee5b5028df3fa0",
      "5c41b0a4d88c48c78488ccbc830b5589",
      "4fa85936af96474b935f6574ee2652e6",
      "aebe9d467a3745848efdf03864fbd7ab",
      "ac93adf03cfa4955b9dac2a29a9acc0e",
      "7fc9761a66e4488a85ed39bc14345c16",
      "67c37a97acf54dd0b9fd4b85cb883eac",
      "9e148caa6c5845aab263dbb86298596e",
      "33d3b78c22ac4e239c653f8ce7a08321"
     ]
    },
    "id": "IcUgpK5piRDq",
    "outputId": "08d1913f-6fb6-4e30-e79f-1c6ad228369b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9301a7b09704f418d642abd507535b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_target = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_iter):\n",
    "        for text, idx in zip(batch.text[0], batch.id):\n",
    "            text = text.unsqueeze(0)\n",
    "            res, _ = model(text, hidden=None)\n",
    "            target = np.round(res.cpu().numpy())\n",
    "            results_target.append(target[0][1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "33d3b78c22ac4e239c653f8ce7a08321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fa85936af96474b935f6574ee2652e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c41b0a4d88c48c78488ccbc830b5589": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e148caa6c5845aab263dbb86298596e",
      "placeholder": "​",
      "style": "IPY_MODEL_33d3b78c22ac4e239c653f8ce7a08321",
      "value": " 102/102 [00:30&lt;00:00,  3.68it/s]"
     }
    },
    "67c37a97acf54dd0b9fd4b85cb883eac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fc9761a66e4488a85ed39bc14345c16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e148caa6c5845aab263dbb86298596e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac93adf03cfa4955b9dac2a29a9acc0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aebe9d467a3745848efdf03864fbd7ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bea7dae81cf84737a5ee5b5028df3fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fc9761a66e4488a85ed39bc14345c16",
      "max": 102,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67c37a97acf54dd0b9fd4b85cb883eac",
      "value": 102
     }
    },
    "c9301a7b09704f418d642abd507535b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed98ee7ab5a34fb9842ec3f5f04c81a9",
       "IPY_MODEL_bea7dae81cf84737a5ee5b5028df3fa0",
       "IPY_MODEL_5c41b0a4d88c48c78488ccbc830b5589"
      ],
      "layout": "IPY_MODEL_4fa85936af96474b935f6574ee2652e6"
     }
    },
    "ed98ee7ab5a34fb9842ec3f5f04c81a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aebe9d467a3745848efdf03864fbd7ab",
      "placeholder": "​",
      "style": "IPY_MODEL_ac93adf03cfa4955b9dac2a29a9acc0e",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
